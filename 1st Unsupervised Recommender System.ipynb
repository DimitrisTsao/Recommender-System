{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c30b94-46dc-424a-aac5-e5435353aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "import pandas as pd\n",
    "products_df = pd.read_csv('products.csv')\n",
    "ratings_df = pd.read_csv('ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f76c62b-2ca9-4856-b07b-52e9c7a7adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of products dataframe are: (60, 3) \n",
      "The dimensions of ratings dataframe are: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "print('The dimensions of products dataframe are:', products_df.shape,'\\nThe dimensions of ratings dataframe are:', ratings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1bd4f9-fef0-4747-990f-1eb77fae8502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>products</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Iphone 13 Pro Max</td>\n",
       "      <td>flagship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Iphone 13 Pro</td>\n",
       "      <td>high_end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Iphone 13</td>\n",
       "      <td>high_end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Iphone 13 Mini</td>\n",
       "      <td>high_end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Iphone 12 Pro Max</td>\n",
       "      <td>high_end</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   productId           products  category\n",
       "0          1  Iphone 13 Pro Max  flagship\n",
       "1          2      Iphone 13 Pro  high_end\n",
       "2          3          Iphone 13  high_end\n",
       "3          4     Iphone 13 Mini  high_end\n",
       "4          5  Iphone 12 Pro Max  high_end"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406baa31-d348-459f-bd5a-ab83307e0538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>productId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>964982931.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>964982176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>964984002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>964982681.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>964984041.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  productId  rating    timestamp\n",
       "0       1          7       3  964982931.0\n",
       "1       2         30       3  964982176.0\n",
       "2       3         51       5  964984002.0\n",
       "3       4         28       2  964982681.0\n",
       "4       5         44       5  964984041.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3160c03-88f0-4d40-9ca2-7ab389cfb522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 1000\n",
      "Number of unique products: 60\n",
      "The full rating matrix will have: 60000 elements.\n",
      "----------\n",
      "Number of ratings: 1000\n",
      "Therefore:  1.6666666666666667 % of the matrix is filled.\n"
     ]
    }
   ],
   "source": [
    "#Products ID to Product name mapping\n",
    "products_names = products_df.set_index('productId')['products'].to_dict()\n",
    "n_users = len(ratings_df.userId.unique())\n",
    "n_items = len(ratings_df.productId.unique())\n",
    "print(\"Number of unique users:\", n_users)\n",
    "print(\"Number of unique products:\", n_items)\n",
    "print(\"The full rating matrix will have:\", n_users*n_items, 'elements.')\n",
    "print('----------')\n",
    "print(\"Number of ratings:\", len(ratings_df))\n",
    "print(\"Therefore: \", len(ratings_df) / (n_users*n_items) * 100, '% of the matrix is filled.')\n",
    "#(\"We have an incredibly sparse matrix to work with here.\")\n",
    "#(\"And... as you can imagine, as the number of users and products grow, the number of elements will increase by n*2\")\n",
    "#(\"You are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a challenge.\")\n",
    "#(\"One advantage here is that matrix factorization can realize the rating matrix implicitly, thus we don't need all the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef04c503-0e88-4029-b771-999b7d622014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. \n",
    "#Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices.\n",
    "\n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create user embeddings\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors) # think of this as a lookup table for the input.\n",
    "        # Create item embeddings\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors) # think of this as a lookup table for the input.\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.item_factors.weight.data.uniform_(0, 0.05)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # Matrix multiplication\n",
    "        users, items = data[:,0], data[:,1]\n",
    "        return (self.user_factors(users)*self.item_factors(items)).sum(1)\n",
    "    # def forward(self, user, item):\n",
    "    # \t# matrix multiplication\n",
    "    #     return (self.user_factors(user)*self.item_factors(item)).sum(1)\n",
    "    \n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4fff65f-9b04-49ac-a898-3ec8ebbec823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataloader (necessary for PyTorch)\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader # package that helps transform your data to machine learning readiness\n",
    "\n",
    "class Loader(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ratings = ratings_df.copy()\n",
    "        \n",
    "        # Extract all user IDs and Product IDs\n",
    "        users = ratings_df.userId.unique()\n",
    "        products = ratings_df.productId.unique()\n",
    "        \n",
    "#self. is the \"tag\" method in Python to refer to instance attributes       \n",
    "\n",
    "        #--- Producing new continuous IDs for Users and Products ---\n",
    "        \n",
    "        # Unique values : index\n",
    "        self.userid2idx = {o:i for i,o in enumerate(users)}\n",
    "        self.productid2idx = {o:i for i,o in enumerate(products)}\n",
    "        \n",
    "        # Obtained continuous ID for Users and Products\n",
    "        self.idx2userid = {i:o for o,i in self.userid2idx.items()}\n",
    "        self.idx2productid = {i:o for o,i in self.productid2idx.items()}\n",
    "        \n",
    "        # Return the id from the indexed values as noted in the lambda function down below.\n",
    "        self.ratings.productId = ratings_df.productId.apply(lambda x: self.productid2idx[x])\n",
    "        self.ratings.userId = ratings_df.userId.apply(lambda x: self.userid2idx[x])\n",
    "        \n",
    "        \n",
    "        self.x = self.ratings.drop(['rating', 'timestamp'], axis=1).values\n",
    "        self.y = self.ratings['rating'].values\n",
    "        self.x, self.y = torch.tensor(self.x), torch.tensor(self.y) # Transforms the data to tensors (ready for torch models.)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00d0af4-3b5b-448f-8ed8-e180649984a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is running on GPU: True\n",
      "MatrixFactorization(\n",
      "  (user_factors): Embedding(1000, 8)\n",
      "  (item_factors): Embedding(60, 8)\n",
      ")\n",
      "user_factors.weight tensor([[0.0127, 0.0428, 0.0190,  ..., 0.0065, 0.0216, 0.0149],\n",
      "        [0.0124, 0.0076, 0.0061,  ..., 0.0237, 0.0024, 0.0069],\n",
      "        [0.0490, 0.0101, 0.0342,  ..., 0.0058, 0.0262, 0.0269],\n",
      "        ...,\n",
      "        [0.0149, 0.0370, 0.0375,  ..., 0.0332, 0.0089, 0.0205],\n",
      "        [0.0172, 0.0001, 0.0373,  ..., 0.0320, 0.0043, 0.0021],\n",
      "        [0.0052, 0.0159, 0.0408,  ..., 0.0230, 0.0131, 0.0354]])\n",
      "item_factors.weight tensor([[0.0071, 0.0324, 0.0195, 0.0218, 0.0406, 0.0470, 0.0402, 0.0003],\n",
      "        [0.0373, 0.0270, 0.0373, 0.0344, 0.0038, 0.0155, 0.0060, 0.0059],\n",
      "        [0.0245, 0.0197, 0.0399, 0.0465, 0.0228, 0.0116, 0.0037, 0.0117],\n",
      "        [0.0374, 0.0493, 0.0290, 0.0427, 0.0068, 0.0233, 0.0184, 0.0484],\n",
      "        [0.0053, 0.0083, 0.0249, 0.0425, 0.0386, 0.0060, 0.0314, 0.0336],\n",
      "        [0.0093, 0.0369, 0.0333, 0.0338, 0.0087, 0.0197, 0.0248, 0.0200],\n",
      "        [0.0171, 0.0353, 0.0254, 0.0153, 0.0282, 0.0405, 0.0453, 0.0034],\n",
      "        [0.0349, 0.0294, 0.0264, 0.0490, 0.0297, 0.0302, 0.0370, 0.0338],\n",
      "        [0.0453, 0.0342, 0.0055, 0.0455, 0.0202, 0.0168, 0.0151, 0.0166],\n",
      "        [0.0312, 0.0119, 0.0486, 0.0035, 0.0144, 0.0081, 0.0483, 0.0235],\n",
      "        [0.0409, 0.0497, 0.0106, 0.0270, 0.0237, 0.0094, 0.0340, 0.0223],\n",
      "        [0.0447, 0.0469, 0.0125, 0.0419, 0.0026, 0.0491, 0.0358, 0.0164],\n",
      "        [0.0009, 0.0155, 0.0148, 0.0479, 0.0457, 0.0053, 0.0256, 0.0484],\n",
      "        [0.0119, 0.0203, 0.0263, 0.0217, 0.0009, 0.0165, 0.0167, 0.0475],\n",
      "        [0.0394, 0.0097, 0.0266, 0.0151, 0.0087, 0.0213, 0.0128, 0.0243],\n",
      "        [0.0437, 0.0245, 0.0221, 0.0231, 0.0149, 0.0140, 0.0169, 0.0266],\n",
      "        [0.0340, 0.0408, 0.0295, 0.0278, 0.0437, 0.0400, 0.0453, 0.0211],\n",
      "        [0.0180, 0.0054, 0.0408, 0.0103, 0.0165, 0.0183, 0.0455, 0.0328],\n",
      "        [0.0245, 0.0319, 0.0429, 0.0395, 0.0035, 0.0146, 0.0436, 0.0250],\n",
      "        [0.0364, 0.0381, 0.0270, 0.0030, 0.0086, 0.0490, 0.0458, 0.0047],\n",
      "        [0.0446, 0.0175, 0.0116, 0.0341, 0.0117, 0.0009, 0.0324, 0.0398],\n",
      "        [0.0319, 0.0019, 0.0062, 0.0280, 0.0484, 0.0270, 0.0141, 0.0073],\n",
      "        [0.0076, 0.0118, 0.0475, 0.0160, 0.0242, 0.0009, 0.0457, 0.0193],\n",
      "        [0.0360, 0.0350, 0.0223, 0.0493, 0.0160, 0.0066, 0.0094, 0.0091],\n",
      "        [0.0016, 0.0106, 0.0160, 0.0457, 0.0211, 0.0185, 0.0354, 0.0262],\n",
      "        [0.0222, 0.0273, 0.0072, 0.0082, 0.0175, 0.0486, 0.0496, 0.0448],\n",
      "        [0.0025, 0.0313, 0.0301, 0.0447, 0.0133, 0.0425, 0.0339, 0.0217],\n",
      "        [0.0289, 0.0267, 0.0052, 0.0051, 0.0081, 0.0113, 0.0464, 0.0440],\n",
      "        [0.0453, 0.0148, 0.0254, 0.0295, 0.0315, 0.0397, 0.0351, 0.0096],\n",
      "        [0.0470, 0.0140, 0.0140, 0.0440, 0.0132, 0.0191, 0.0220, 0.0326],\n",
      "        [0.0125, 0.0336, 0.0153, 0.0249, 0.0149, 0.0313, 0.0302, 0.0466],\n",
      "        [0.0133, 0.0006, 0.0422, 0.0439, 0.0349, 0.0004, 0.0478, 0.0307],\n",
      "        [0.0235, 0.0469, 0.0337, 0.0098, 0.0316, 0.0228, 0.0050, 0.0388],\n",
      "        [0.0411, 0.0309, 0.0284, 0.0250, 0.0251, 0.0332, 0.0438, 0.0223],\n",
      "        [0.0114, 0.0329, 0.0379, 0.0313, 0.0492, 0.0179, 0.0029, 0.0057],\n",
      "        [0.0228, 0.0463, 0.0408, 0.0373, 0.0030, 0.0164, 0.0477, 0.0210],\n",
      "        [0.0390, 0.0183, 0.0040, 0.0141, 0.0403, 0.0129, 0.0136, 0.0395],\n",
      "        [0.0047, 0.0118, 0.0288, 0.0264, 0.0132, 0.0351, 0.0095, 0.0239],\n",
      "        [0.0312, 0.0168, 0.0061, 0.0008, 0.0391, 0.0368, 0.0452, 0.0408],\n",
      "        [0.0359, 0.0440, 0.0099, 0.0443, 0.0078, 0.0119, 0.0314, 0.0481],\n",
      "        [0.0372, 0.0129, 0.0428, 0.0340, 0.0415, 0.0499, 0.0138, 0.0384],\n",
      "        [0.0195, 0.0047, 0.0439, 0.0083, 0.0130, 0.0205, 0.0099, 0.0198],\n",
      "        [0.0401, 0.0059, 0.0143, 0.0092, 0.0037, 0.0031, 0.0257, 0.0300],\n",
      "        [0.0251, 0.0422, 0.0131, 0.0201, 0.0197, 0.0371, 0.0148, 0.0158],\n",
      "        [0.0042, 0.0166, 0.0492, 0.0411, 0.0248, 0.0021, 0.0321, 0.0203],\n",
      "        [0.0165, 0.0352, 0.0285, 0.0476, 0.0341, 0.0178, 0.0262, 0.0024],\n",
      "        [0.0249, 0.0012, 0.0419, 0.0169, 0.0244, 0.0455, 0.0489, 0.0453],\n",
      "        [0.0257, 0.0076, 0.0103, 0.0358, 0.0055, 0.0026, 0.0093, 0.0040],\n",
      "        [0.0286, 0.0413, 0.0059, 0.0076, 0.0034, 0.0383, 0.0097, 0.0074],\n",
      "        [0.0305, 0.0231, 0.0055, 0.0140, 0.0275, 0.0161, 0.0402, 0.0013],\n",
      "        [0.0469, 0.0216, 0.0378, 0.0179, 0.0350, 0.0392, 0.0350, 0.0444],\n",
      "        [0.0151, 0.0015, 0.0209, 0.0269, 0.0237, 0.0297, 0.0230, 0.0185],\n",
      "        [0.0264, 0.0010, 0.0465, 0.0002, 0.0215, 0.0038, 0.0442, 0.0084],\n",
      "        [0.0177, 0.0057, 0.0106, 0.0087, 0.0149, 0.0150, 0.0263, 0.0208],\n",
      "        [0.0364, 0.0293, 0.0159, 0.0106, 0.0027, 0.0019, 0.0215, 0.0241],\n",
      "        [0.0021, 0.0147, 0.0454, 0.0111, 0.0002, 0.0069, 0.0332, 0.0355],\n",
      "        [0.0345, 0.0314, 0.0489, 0.0299, 0.0036, 0.0467, 0.0048, 0.0021],\n",
      "        [0.0079, 0.0012, 0.0203, 0.0142, 0.0483, 0.0152, 0.0149, 0.0126],\n",
      "        [0.0096, 0.0294, 0.0372, 0.0239, 0.0366, 0.0266, 0.0102, 0.0133],\n",
      "        [0.0242, 0.0411, 0.0402, 0.0376, 0.0035, 0.0147, 0.0438, 0.0188]])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 128\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Is running on GPU:\", cuda)\n",
    "\n",
    "model = MatrixFactorization(n_users, n_items, n_factors=8)\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "# GPU enable if you have a GPU...\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "# The three steps to building a prototype: \n",
    "# 1) defining the model, \n",
    "# 2) defining the loss,\n",
    "# 3) and picking an optimization technique. The latter two steps are largely built into PyTorch, so we’ll start with the hardest first.\n",
    "\n",
    "# MSE loss (MEAN SQUARE ERROR: loss between the matrix factorization “prediction” and the actual user-item ratings)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "#adagrad_loss = torch.optim.Adagrad(model.parameters(), lr= 1e-3) #different optimization algorithm\n",
    "\n",
    "#Me lr=1e-1 exoume to xamhlotero error sthn teleutaia epoch\n",
    "# ADAM optimizier \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) #learnig rate\n",
    "\n",
    "# Train data\n",
    "train_set = Loader()\n",
    "train_loader = DataLoader(train_set, 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7632b6eb-c943-493f-a44e-d0d652dad5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makis\\AppData\\Local\\Temp/ipykernel_38576/1510178570.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for it in tqdm(range(num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c40122e1434fc2bff5b302b801bc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter #0 Loss: 11.005945801734924\n",
      "iter #1 Loss: 10.959268450737\n",
      "iter #2 Loss: 10.968974947929382\n",
      "iter #3 Loss: 10.905981063842773\n",
      "iter #4 Loss: 10.856761932373047\n",
      "iter #5 Loss: 10.880078673362732\n",
      "iter #6 Loss: 10.853960394859314\n",
      "iter #7 Loss: 10.811119079589844\n",
      "iter #8 Loss: 10.778462409973145\n",
      "iter #9 Loss: 10.723978400230408\n",
      "iter #10 Loss: 10.691505312919617\n",
      "iter #11 Loss: 10.704487085342407\n",
      "iter #12 Loss: 10.631688117980957\n",
      "iter #13 Loss: 10.545263528823853\n",
      "iter #14 Loss: 10.520253539085388\n",
      "iter #15 Loss: 10.473262071609497\n",
      "iter #16 Loss: 10.388989567756653\n",
      "iter #17 Loss: 10.303861618041992\n",
      "iter #18 Loss: 10.248422384262085\n",
      "iter #19 Loss: 10.227567315101624\n",
      "iter #20 Loss: 10.12908399105072\n",
      "iter #21 Loss: 9.993457913398743\n",
      "iter #22 Loss: 9.945315480232239\n",
      "iter #23 Loss: 9.878607153892517\n",
      "iter #24 Loss: 9.771252751350403\n",
      "iter #25 Loss: 9.659219145774841\n",
      "iter #26 Loss: 9.530373811721802\n",
      "iter #27 Loss: 9.466899156570435\n",
      "iter #28 Loss: 9.342447519302368\n",
      "iter #29 Loss: 9.209536254405975\n",
      "iter #30 Loss: 9.115285634994507\n",
      "iter #31 Loss: 9.017919898033142\n",
      "iter #32 Loss: 8.889305114746094\n",
      "iter #33 Loss: 8.770970821380615\n",
      "iter #34 Loss: 8.673780500888824\n",
      "iter #35 Loss: 8.521317720413208\n",
      "iter #36 Loss: 8.42781776189804\n",
      "iter #37 Loss: 8.287368357181549\n",
      "iter #38 Loss: 8.179320394992828\n",
      "iter #39 Loss: 8.020629167556763\n",
      "iter #40 Loss: 7.894441843032837\n",
      "iter #41 Loss: 7.774011790752411\n",
      "iter #42 Loss: 7.636305570602417\n",
      "iter #43 Loss: 7.477655708789825\n",
      "iter #44 Loss: 7.341671288013458\n",
      "iter #45 Loss: 7.2568172216415405\n",
      "iter #46 Loss: 7.095106780529022\n",
      "iter #47 Loss: 6.977044522762299\n",
      "iter #48 Loss: 6.850649833679199\n",
      "iter #49 Loss: 6.66267728805542\n",
      "iter #50 Loss: 6.574502348899841\n",
      "iter #51 Loss: 6.427558124065399\n",
      "iter #52 Loss: 6.27837860584259\n",
      "iter #53 Loss: 6.143244385719299\n",
      "iter #54 Loss: 6.023047089576721\n",
      "iter #55 Loss: 5.88178688287735\n",
      "iter #56 Loss: 5.754692614078522\n",
      "iter #57 Loss: 5.641844749450684\n",
      "iter #58 Loss: 5.506670236587524\n",
      "iter #59 Loss: 5.373515427112579\n",
      "iter #60 Loss: 5.242782652378082\n",
      "iter #61 Loss: 5.097553968429565\n",
      "iter #62 Loss: 4.957379400730133\n",
      "iter #63 Loss: 4.830209314823151\n",
      "iter #64 Loss: 4.730573296546936\n",
      "iter #65 Loss: 4.603772342205048\n",
      "iter #66 Loss: 4.496251106262207\n",
      "iter #67 Loss: 4.375314086675644\n",
      "iter #68 Loss: 4.24542036652565\n",
      "iter #69 Loss: 4.148730635643005\n",
      "iter #70 Loss: 4.015893638134003\n",
      "iter #71 Loss: 3.8873805105686188\n",
      "iter #72 Loss: 3.77640163898468\n",
      "iter #73 Loss: 3.681556135416031\n",
      "iter #74 Loss: 3.5659040808677673\n",
      "iter #75 Loss: 3.4657842814922333\n",
      "iter #76 Loss: 3.3580597937107086\n",
      "iter #77 Loss: 3.2713491022586823\n",
      "iter #78 Loss: 3.1753175854682922\n",
      "iter #79 Loss: 3.052691638469696\n",
      "iter #80 Loss: 2.968272179365158\n",
      "iter #81 Loss: 2.884725332260132\n",
      "iter #82 Loss: 2.775756388902664\n",
      "iter #83 Loss: 2.6934869587421417\n",
      "iter #84 Loss: 2.6023254990577698\n",
      "iter #85 Loss: 2.5074703991413116\n",
      "iter #86 Loss: 2.434561103582382\n",
      "iter #87 Loss: 2.3440365195274353\n",
      "iter #88 Loss: 2.271054208278656\n",
      "iter #89 Loss: 2.1985836774110794\n",
      "iter #90 Loss: 2.114227294921875\n",
      "iter #91 Loss: 2.0381798297166824\n",
      "iter #92 Loss: 1.9626432359218597\n",
      "iter #93 Loss: 1.9061247408390045\n",
      "iter #94 Loss: 1.8260622471570969\n",
      "iter #95 Loss: 1.7624670714139938\n",
      "iter #96 Loss: 1.6997213810682297\n",
      "iter #97 Loss: 1.640282765030861\n",
      "iter #98 Loss: 1.5795553475618362\n",
      "iter #99 Loss: 1.5223478078842163\n",
      "iter #100 Loss: 1.4627391397953033\n",
      "iter #101 Loss: 1.4046906381845474\n",
      "iter #102 Loss: 1.354301005601883\n",
      "iter #103 Loss: 1.302106261253357\n",
      "iter #104 Loss: 1.2476514726877213\n",
      "iter #105 Loss: 1.2004977762699127\n",
      "iter #106 Loss: 1.1535079777240753\n",
      "iter #107 Loss: 1.1106839552521706\n",
      "iter #108 Loss: 1.0632078349590302\n",
      "iter #109 Loss: 1.0215625017881393\n",
      "iter #110 Loss: 0.9767956882715225\n",
      "iter #111 Loss: 0.935090459883213\n",
      "iter #112 Loss: 0.8969361335039139\n",
      "iter #113 Loss: 0.8643106669187546\n",
      "iter #114 Loss: 0.8304959908127785\n",
      "iter #115 Loss: 0.7922477424144745\n",
      "iter #116 Loss: 0.7602979391813278\n",
      "iter #117 Loss: 0.727226547896862\n",
      "iter #118 Loss: 0.6987246200442314\n",
      "iter #119 Loss: 0.673382081091404\n",
      "iter #120 Loss: 0.6359278075397015\n",
      "iter #121 Loss: 0.6132866591215134\n",
      "iter #122 Loss: 0.5837106928229332\n",
      "iter #123 Loss: 0.5622815974056721\n",
      "iter #124 Loss: 0.53118646889925\n",
      "iter #125 Loss: 0.5157899484038353\n",
      "iter #126 Loss: 0.4890332296490669\n",
      "iter #127 Loss: 0.4692738465964794\n"
     ]
    }
   ],
   "source": [
    "#TQDM is used for creating Progress Meters or Progress Bars in Python.\n",
    "for it in tqdm(range(num_epochs)):\n",
    "    losses = []\n",
    "    for x, y in train_loader:\n",
    "         if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(\"iter #{}\".format(it), \"Loss:\", sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba51340d-d450-4357-9e97-65aaa53f8ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_factors.weight tensor([[0.4058, 0.4196, 0.4025,  ..., 0.3752, 0.3943, 0.4108],\n",
      "        [0.3888, 0.3902, 0.3833,  ..., 0.4143, 0.3998, 0.4027],\n",
      "        [0.5033, 0.4673, 0.4818,  ..., 0.4678, 0.4955, 0.4884],\n",
      "        ...,\n",
      "        [0.2699, 0.3098, 0.3043,  ..., 0.3083, 0.2702, 0.2805],\n",
      "        [0.4427, 0.4294, 0.4772,  ..., 0.4657, 0.4247, 0.4445],\n",
      "        [0.1344, 0.1395, 0.1567,  ..., 0.1492, 0.1313, 0.1527]],\n",
      "       device='cuda:0')\n",
      "item_factors.weight tensor([[0.9098, 0.9366, 0.9201, 0.9074, 0.9548, 0.9449, 0.9352, 0.9136],\n",
      "        [0.9284, 0.9113, 0.9208, 0.9377, 0.8994, 0.8837, 0.8912, 0.8901],\n",
      "        [1.0110, 1.0201, 1.0365, 1.0472, 1.0051, 0.9944, 0.9923, 1.0040],\n",
      "        [0.9709, 0.9624, 0.9457, 0.9578, 0.9320, 0.9506, 0.9421, 0.9675],\n",
      "        [0.9616, 1.0077, 0.9957, 1.0287, 1.0025, 0.9747, 1.0042, 1.0174],\n",
      "        [0.8592, 0.8851, 0.9049, 0.8781, 0.8583, 0.8758, 0.8863, 0.8742],\n",
      "        [0.9861, 1.0129, 1.0099, 1.0086, 1.0011, 1.0302, 1.0241, 0.9872],\n",
      "        [0.8987, 0.8898, 0.8963, 0.9144, 0.8849, 0.9227, 0.9117, 0.8971],\n",
      "        [0.9593, 0.9601, 0.9297, 0.9700, 0.9414, 0.9287, 0.9326, 0.9326],\n",
      "        [0.9488, 0.9367, 0.9627, 0.9066, 0.9277, 0.9186, 0.9604, 0.9540],\n",
      "        [0.8937, 0.9245, 0.9024, 0.9076, 0.9187, 0.8937, 0.9182, 0.9132],\n",
      "        [0.9387, 0.9448, 0.9024, 0.9523, 0.9040, 0.9454, 0.9297, 0.9079],\n",
      "        [0.9629, 0.9836, 1.0018, 1.0080, 1.0046, 0.9737, 0.9745, 1.0246],\n",
      "        [0.9634, 0.9883, 0.9796, 0.9702, 0.9577, 0.9851, 0.9815, 0.9959],\n",
      "        [0.9363, 0.9196, 0.9302, 0.9401, 0.9356, 0.9215, 0.9215, 0.9159],\n",
      "        [0.8924, 0.8585, 0.8522, 0.8733, 0.8428, 0.8775, 0.8644, 0.8854],\n",
      "        [0.9959, 0.9829, 0.9923, 0.9642, 1.0002, 1.0061, 0.9849, 0.9846],\n",
      "        [0.9607, 0.9586, 0.9716, 0.9642, 0.9529, 0.9581, 0.9746, 0.9850],\n",
      "        [0.8517, 0.8747, 0.8920, 0.8835, 0.8425, 0.8633, 0.8853, 0.8741],\n",
      "        [0.9224, 0.9428, 0.9241, 0.8958, 0.8930, 0.9475, 0.9543, 0.8984],\n",
      "        [0.9198, 0.8875, 0.8960, 0.9146, 0.8818, 0.8547, 0.9068, 0.9052],\n",
      "        [0.8855, 0.8701, 0.8670, 0.8957, 0.9091, 0.8833, 0.8698, 0.8859],\n",
      "        [0.9709, 0.9862, 1.0030, 0.9687, 0.9916, 0.9557, 0.9918, 0.9948],\n",
      "        [0.9779, 0.9660, 0.9583, 0.9900, 0.9534, 0.9411, 0.9338, 0.9287],\n",
      "        [0.9260, 0.9267, 0.9196, 0.9330, 0.9389, 0.9382, 0.9417, 0.9484],\n",
      "        [0.9787, 0.9664, 0.9612, 0.9344, 0.9562, 0.9830, 0.9830, 0.9851],\n",
      "        [0.9237, 0.9517, 0.9625, 0.9667, 0.9546, 0.9672, 0.9512, 0.9294],\n",
      "        [0.9526, 0.9344, 0.9100, 0.9240, 0.9193, 0.9290, 0.9634, 0.9582],\n",
      "        [0.9748, 0.9409, 0.9420, 0.9614, 0.9548, 0.9609, 0.9460, 0.9442],\n",
      "        [0.8727, 0.8445, 0.8413, 0.8815, 0.8371, 0.8379, 0.8490, 0.8646],\n",
      "        [0.9191, 0.9266, 0.9240, 0.9383, 0.9291, 0.9305, 0.9527, 0.9470],\n",
      "        [0.9576, 0.9384, 0.9741, 0.9991, 0.9802, 0.9446, 0.9828, 0.9687],\n",
      "        [0.8844, 0.9165, 0.8933, 0.8749, 0.8898, 0.8943, 0.8599, 0.8982],\n",
      "        [0.8600, 0.8590, 0.8755, 0.8712, 0.8588, 0.8552, 0.8634, 0.8449],\n",
      "        [0.8921, 0.9465, 0.9126, 0.9254, 0.9216, 0.8900, 0.8883, 0.8899],\n",
      "        [0.8777, 0.9053, 0.9077, 0.9119, 0.8695, 0.8882, 0.9016, 0.8764],\n",
      "        [0.9382, 0.9205, 0.8999, 0.9334, 0.9487, 0.9147, 0.8960, 0.9358],\n",
      "        [0.8128, 0.8196, 0.8266, 0.8642, 0.8230, 0.8387, 0.8191, 0.8601],\n",
      "        [0.9605, 0.9166, 0.8962, 0.8875, 0.9305, 0.9614, 0.9642, 0.9453],\n",
      "        [0.9938, 1.0050, 0.9904, 0.9961, 0.9560, 0.9656, 1.0001, 1.0085],\n",
      "        [0.9045, 0.8771, 0.9332, 0.8910, 0.9167, 0.9230, 0.8742, 0.8921],\n",
      "        [0.9900, 0.9667, 1.0192, 0.9603, 0.9814, 0.9876, 0.9692, 0.9734],\n",
      "        [0.9524, 0.9317, 0.9210, 0.9271, 0.9230, 0.9174, 0.9482, 0.9631],\n",
      "        [0.9616, 0.9576, 0.9204, 0.9398, 0.9338, 0.9473, 0.9362, 0.9580],\n",
      "        [0.9199, 0.9323, 0.9699, 0.9766, 0.9332, 0.9184, 0.9539, 0.9472],\n",
      "        [0.8351, 0.8502, 0.8541, 0.8612, 0.8609, 0.8624, 0.8477, 0.8237],\n",
      "        [0.9076, 0.8854, 0.9212, 0.8937, 0.8862, 0.9320, 0.9239, 0.9409],\n",
      "        [0.8992, 0.8669, 0.8833, 0.9050, 0.8953, 0.8629, 0.8880, 0.8617],\n",
      "        [0.9590, 0.9815, 0.9666, 0.9508, 0.9355, 0.9829, 0.9759, 0.9506],\n",
      "        [0.9958, 0.9742, 0.9705, 0.9760, 0.9934, 0.9733, 1.0099, 0.9524],\n",
      "        [0.8923, 0.8492, 0.8933, 0.8753, 0.8952, 0.8850, 0.8840, 0.8799],\n",
      "        [0.8808, 0.8418, 0.8559, 0.8868, 0.8750, 0.9233, 0.8600, 0.8616],\n",
      "        [0.9558, 0.9313, 0.9793, 0.9344, 0.9551, 0.9341, 0.9850, 0.9474],\n",
      "        [0.8233, 0.8172, 0.8321, 0.8328, 0.8364, 0.8322, 0.8368, 0.8260],\n",
      "        [0.8746, 0.9105, 0.8542, 0.8543, 0.8530, 0.8467, 0.8855, 0.8546],\n",
      "        [0.8264, 0.8534, 0.9075, 0.8541, 0.8393, 0.8161, 0.8809, 0.8862],\n",
      "        [0.9393, 0.9391, 0.9505, 0.9348, 0.8936, 0.9389, 0.8930, 0.9030],\n",
      "        [0.8164, 0.8154, 0.8536, 0.8283, 0.8806, 0.8392, 0.8491, 0.8186],\n",
      "        [0.9011, 0.9469, 0.9374, 0.9427, 0.9333, 0.9250, 0.8969, 0.9108],\n",
      "        [0.9470, 0.9584, 0.9543, 0.9600, 0.9258, 0.9206, 0.9586, 0.9533]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# By training the model, we will have tuned latent factors for products and users. \n",
    "# Latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable\n",
    "\n",
    "c = 0\n",
    "uw = 0\n",
    "iw = 0 \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "        if c == 0:\n",
    "          uw = param.data\n",
    "          c +=1\n",
    "        else:\n",
    "          iw = param.data\n",
    "        #print('param_data', param_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9311e5b-5dd0-4bba-b91f-d1b3d30d4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_products_embeddings = model.item_factors.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2a23412-337f-4031-b817-8d6d106174a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trained_products_embeddings) # Unique products factor weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c295d43-8bcf-410a-97ce-478ed024ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Fit the clusters based on the products weights\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(trained_products_embeddings) #Orismos CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e4a5ee-482b-4355-9863-e70e32c18924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0\n",
      "\t Iphone 12 Mini\n",
      "\t Huawei P50\n",
      "\t One Plus 8 Pro\n",
      "\t Samsung Galaxy S20 Plus\n",
      "\t Samsung Galaxy S21 Ultra\n",
      "\t One Plus 8T\n",
      "\t Realme 9 Pro\n",
      "\t One Plus Nord 2\n",
      "\t Xiaomi Redmi Note 10\n",
      "\t Google Pixel 5\n",
      "Cluster #1\n",
      "\t Google Pixel 6a\n",
      "\t Realme 9 Pro Plus\n",
      "\t Realme GT Neo 2\n",
      "\t Huawei P40 Pro\n",
      "\t Realme Gt Neo 3 Pro\n",
      "\t Samsung Galaxy S22 Ultra\n",
      "\t Xiaomi 12 Pro\n",
      "\t One Plus 9 Pro\n",
      "Cluster #2\n",
      "\t Samsung Galaxy S21\n",
      "\t Xiaomi Poco M4 Pro\n",
      "\t Xiaomi Redmi Note 11 Pro\n",
      "\t Iphone 11 Pro\n",
      "\t Xiaomi 12X\n",
      "\t Huawei P40\n",
      "\t Google Pixel 5a\n",
      "\t Samsung Galaxy S20 Fe\n",
      "\t Xiaomi Poco F3\n",
      "\t Huawei P40 lite\n",
      "Cluster #3\n",
      "\t Iphone 13 Mini\n",
      "\t Iphone 13 Pro\n",
      "\t Huawei 8i\n",
      "\t Xiaomi Redmi Note 11\n",
      "\t Samsung Galaxy S22 Plus\n",
      "\t Iphone 11 Pro Max\n",
      "\t Xiaomi Redmi Note 9 Pro\n",
      "\t Realme GT 2\n",
      "\t Huawei P30 Pro\n",
      "\t Iphone 13 Pro Max\n"
     ]
    }
   ],
   "source": [
    "'''It can be seen here that the products that are in the same cluster tend to have\n",
    "similar categories. #\"THA EPREPE NA EXEI TIS IDIES KATHGORIES\" \n",
    "Also note that the algorithm is unfamiliar with the product name\n",
    "and only obtained the relationships by looking at the numbers representing how\n",
    "users have responded to the products selections.'''\n",
    "\n",
    "#Epileksame na orisoume 4 clusters epeidh einai 4 kai oi kathgories twn proiontwn mas (Flagship, High End, Mid Range & Low End)\n",
    "\n",
    "for cluster in range(4):\n",
    "  print(\"Cluster #{}\".format(cluster))\n",
    "  prods = []\n",
    "  for prodidx in np.where(kmeans.labels_ == cluster)[0]:\n",
    "    prodid = train_set.idx2productid[prodidx]\n",
    "    rat_count = ratings_df.loc[ratings_df['productId']==prodid].count()[0]\n",
    "    prods.append((products_names[prodid], rat_count))\n",
    "  for prod in sorted(prods, key=lambda tup: tup[1], reverse=True)[:10]:\n",
    "    print(\"\\t\", prod[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7a638-7dd5-4397-bd26-5f85ba2c4ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0dcd15-c782-4007-8c3c-541a944a2a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
