{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c30b94-46dc-424a-aac5-e5435353aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset for our needs \n",
    "import pandas as pd\n",
    "\n",
    "#We import the csv \n",
    "products_df = pd.read_csv('products.csv')\n",
    "ratings_df = pd.read_csv('ratings_100k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f76c62b-2ca9-4856-b07b-52e9c7a7adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of products dataframe are: (60, 3) \n",
      "The dimensions of ratings dataframe are: (100000, 4)\n"
     ]
    }
   ],
   "source": [
    "#We inform the costumer about the dimensions of the dataframes \n",
    "print('The dimensions of products dataframe are:', products_df.shape,'\\nThe dimensions of ratings dataframe are:', ratings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1bd4f9-fef0-4747-990f-1eb77fae8502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>products</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Iphone 13 Pro Max</td>\n",
       "      <td>flagship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Iphone 13 Pro</td>\n",
       "      <td>high_end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Iphone 13</td>\n",
       "      <td>high_end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Iphone 13 Mini</td>\n",
       "      <td>high_end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Iphone 12 Pro Max</td>\n",
       "      <td>high_end</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   productId           products  category\n",
       "0          1  Iphone 13 Pro Max  flagship\n",
       "1          2      Iphone 13 Pro  high_end\n",
       "2          3          Iphone 13  high_end\n",
       "3          4     Iphone 13 Mini  high_end\n",
       "4          5  Iphone 12 Pro Max  high_end"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406baa31-d348-459f-bd5a-ab83307e0538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>productId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>964982176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>964984002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>964982681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>964984041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  productId  rating  timestamp\n",
       "0       1          7       3  964982931\n",
       "1       2         30       3  964982176\n",
       "2       3         51       5  964984002\n",
       "3       4         28       2  964982681\n",
       "4       5         44       5  964984041"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3160c03-88f0-4d40-9ca2-7ab389cfb522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 100000\n",
      "Number of unique products: 60\n",
      "The full rating matrix will have: 6000000 elements.\n",
      "----------\n",
      "Number of ratings: 100000\n",
      "Therefore:  1.6666666666666667 % of the matrix is filled.\n"
     ]
    }
   ],
   "source": [
    "#Products ID to Product name mapping\n",
    "products_names = products_df.set_index('productId')['products'].to_dict()\n",
    "n_users = len(ratings_df.userId.unique())\n",
    "n_items = len(ratings_df.productId.unique())\n",
    "print(\"Number of unique users:\", n_users)\n",
    "print(\"Number of unique products:\", n_items)\n",
    "print(\"The full rating matrix will have:\", n_users*n_items, 'elements.')\n",
    "print('----------')\n",
    "print(\"Number of ratings:\", len(ratings_df))\n",
    "print(\"Therefore: \", len(ratings_df) / (n_users*n_items) * 100, '% of the matrix is filled.')\n",
    "#\"We have a sparse matrix to work with here\n",
    "#\"And as the number of users and products grow, the number of elements will increase by n*2\"\n",
    "#\"You are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a issue.\"\n",
    "#\"One advantage here is that matrix factorization can realize the rating matrix implicitly, \n",
    "# thus we don't need all the data to work with \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef04c503-0e88-4029-b771-999b7d622014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch library provides a wide range of algorithms for deep learning\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#Matrix factorization is a class of collaborative filtering algorithms used in a few recommender systems. \n",
    "#Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower \n",
    "#dimensionality rectangular matrices.\n",
    "\n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create user embeddings\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors) # think of this as a lookup table for the input.\n",
    "        # Create item embeddings\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors) # think of this as a lookup table for the input.\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.item_factors.weight.data.uniform_(0, 0.05)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # Matrix multiplication\n",
    "        users, items = data[:,0], data[:,1]\n",
    "        return (self.user_factors(users)*self.item_factors(items)).sum(1)\n",
    "    # def forward(self, user, item):\n",
    "    # \t# matrix multiplication\n",
    "    #     return (self.user_factors(user)*self.item_factors(item)).sum(1)\n",
    "    \n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4fff65f-9b04-49ac-a898-3ec8ebbec823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataloader (necessary for PyTorch)\n",
    "from torch.utils.data.dataset import Dataset\n",
    "# package that helps transform your data to machine learning readiness\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "class Loader(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ratings = ratings_df.copy()\n",
    "        \n",
    "        # Extract all user IDs and Product IDs\n",
    "        users = ratings_df.userId.unique()\n",
    "        products = ratings_df.productId.unique()\n",
    "        \n",
    "#self. is the \"tag\" method in Python to refer to instance attributes       \n",
    "\n",
    "        #--- Producing new continuous IDs for Users and Products ---\n",
    "        \n",
    "        # Unique values : index\n",
    "        self.userid2idx = {o:i for i,o in enumerate(users)}\n",
    "        self.productid2idx = {o:i for i,o in enumerate(products)}\n",
    "        \n",
    "        # Obtained continuous ID for Users and Products\n",
    "        self.idx2userid = {i:o for o,i in self.userid2idx.items()}\n",
    "        self.idx2productid = {i:o for o,i in self.productid2idx.items()}\n",
    "        \n",
    "        # Return the id from the indexed values as noted in the lambda function down below.\n",
    "        self.ratings.productId = ratings_df.productId.apply(lambda x: self.productid2idx[x])\n",
    "        self.ratings.userId = ratings_df.userId.apply(lambda x: self.userid2idx[x])\n",
    "        \n",
    "        \n",
    "        self.x = self.ratings.drop(['rating', 'timestamp'], axis=1).values\n",
    "        self.y = self.ratings['rating'].values\n",
    "        self.x, self.y = torch.tensor(self.x), torch.tensor(self.y) # Transforms the data to tensors (ready for torch models.)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00d0af4-3b5b-448f-8ed8-e180649984a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is running on GPU: True\n",
      "MatrixFactorization(\n",
      "  (user_factors): Embedding(100000, 8)\n",
      "  (item_factors): Embedding(60, 8)\n",
      ")\n",
      "user_factors.weight tensor([[0.0236, 0.0346, 0.0169,  ..., 0.0424, 0.0201, 0.0100],\n",
      "        [0.0462, 0.0436, 0.0343,  ..., 0.0172, 0.0059, 0.0200],\n",
      "        [0.0478, 0.0220, 0.0352,  ..., 0.0137, 0.0282, 0.0427],\n",
      "        ...,\n",
      "        [0.0331, 0.0412, 0.0322,  ..., 0.0159, 0.0378, 0.0171],\n",
      "        [0.0010, 0.0479, 0.0409,  ..., 0.0202, 0.0461, 0.0434],\n",
      "        [0.0154, 0.0329, 0.0344,  ..., 0.0384, 0.0483, 0.0142]])\n",
      "item_factors.weight tensor([[3.1374e-02, 1.1103e-02, 1.3431e-02, 6.5954e-03, 4.6274e-02, 2.2526e-02,\n",
      "         1.0400e-02, 3.4209e-02],\n",
      "        [3.7059e-02, 1.9746e-02, 1.4666e-02, 2.1866e-02, 4.3412e-03, 3.6595e-02,\n",
      "         2.5914e-02, 4.8392e-02],\n",
      "        [4.3732e-02, 2.2399e-02, 5.1709e-03, 1.7975e-02, 4.8494e-02, 2.2326e-02,\n",
      "         2.9496e-02, 8.9814e-04],\n",
      "        [2.9339e-02, 1.9221e-02, 2.7852e-02, 8.7793e-03, 4.6504e-02, 4.8392e-02,\n",
      "         1.2093e-02, 4.7981e-02],\n",
      "        [3.4056e-02, 3.9113e-02, 2.6101e-02, 2.3451e-02, 2.9493e-02, 2.7347e-02,\n",
      "         2.9349e-02, 3.3356e-02],\n",
      "        [3.0586e-02, 4.2817e-02, 4.7156e-02, 4.4558e-02, 4.7339e-02, 3.1254e-02,\n",
      "         3.3496e-02, 2.9600e-02],\n",
      "        [2.6026e-02, 4.2357e-02, 8.9875e-03, 7.8776e-03, 3.8491e-02, 3.3743e-02,\n",
      "         9.9822e-04, 3.2461e-02],\n",
      "        [3.3572e-03, 4.3440e-02, 4.8463e-02, 8.8062e-03, 4.0897e-02, 1.2059e-03,\n",
      "         4.1393e-02, 3.4853e-02],\n",
      "        [3.6375e-02, 4.6634e-02, 1.1222e-02, 2.4186e-02, 4.8354e-03, 3.6594e-02,\n",
      "         1.0657e-02, 4.9050e-02],\n",
      "        [4.9974e-02, 2.5003e-02, 3.7939e-02, 3.0963e-02, 6.2090e-03, 2.5400e-02,\n",
      "         1.4671e-02, 4.2826e-02],\n",
      "        [2.6228e-02, 3.8594e-02, 3.4537e-02, 2.0601e-02, 2.6214e-02, 5.8518e-03,\n",
      "         3.6967e-03, 2.5435e-02],\n",
      "        [3.3883e-02, 4.2001e-02, 4.4468e-02, 2.4473e-03, 2.9575e-02, 2.5155e-02,\n",
      "         2.5892e-02, 2.6589e-02],\n",
      "        [5.2760e-03, 3.6814e-02, 3.2737e-02, 4.5711e-02, 3.1116e-02, 7.1467e-03,\n",
      "         1.8138e-02, 3.0677e-02],\n",
      "        [2.5673e-03, 3.4170e-02, 4.9973e-02, 1.8533e-02, 1.4798e-02, 8.3328e-03,\n",
      "         1.3982e-02, 2.8570e-02],\n",
      "        [2.2115e-02, 2.5953e-02, 3.0117e-02, 1.2926e-02, 5.6487e-03, 4.4538e-02,\n",
      "         4.4621e-02, 3.8975e-02],\n",
      "        [2.9757e-02, 2.8401e-02, 2.8298e-02, 4.8288e-02, 3.2890e-02, 1.2125e-02,\n",
      "         2.3290e-02, 5.2200e-03],\n",
      "        [3.9394e-02, 2.0352e-02, 4.7705e-02, 4.3620e-02, 1.5971e-02, 2.1385e-02,\n",
      "         1.2341e-02, 2.0330e-02],\n",
      "        [2.5358e-02, 3.2807e-03, 2.6877e-02, 3.0390e-02, 2.6091e-02, 3.0819e-02,\n",
      "         2.2320e-02, 2.2275e-02],\n",
      "        [5.3393e-03, 4.6615e-02, 4.0948e-02, 6.6961e-03, 2.4779e-03, 2.7844e-02,\n",
      "         4.4117e-02, 9.6387e-03],\n",
      "        [1.1042e-02, 9.3684e-03, 1.5047e-02, 2.2722e-02, 4.2182e-03, 3.0255e-02,\n",
      "         2.1346e-02, 2.7429e-02],\n",
      "        [3.6583e-02, 4.2191e-04, 4.7011e-02, 4.5499e-02, 2.1299e-02, 1.4237e-02,\n",
      "         2.5940e-02, 2.1354e-03],\n",
      "        [1.1294e-02, 4.5957e-02, 4.7119e-02, 2.1981e-02, 2.9042e-02, 4.2416e-02,\n",
      "         3.2101e-03, 1.5075e-02],\n",
      "        [1.1790e-02, 3.1028e-02, 3.3232e-02, 4.8041e-02, 4.1603e-02, 9.9372e-04,\n",
      "         1.1352e-02, 2.2287e-02],\n",
      "        [2.3634e-02, 4.5693e-03, 2.3956e-02, 2.6313e-02, 1.1988e-03, 4.6119e-02,\n",
      "         4.8715e-02, 3.3996e-02],\n",
      "        [3.0658e-02, 4.1366e-02, 4.9553e-03, 1.6788e-02, 7.2638e-03, 2.1903e-02,\n",
      "         2.5402e-03, 1.9047e-02],\n",
      "        [1.4733e-02, 3.7374e-03, 4.8413e-02, 6.4383e-04, 2.1738e-03, 1.8263e-02,\n",
      "         4.3721e-02, 1.1150e-04],\n",
      "        [4.0272e-02, 2.5370e-02, 3.3056e-02, 3.1042e-02, 4.6470e-02, 3.8077e-02,\n",
      "         3.8882e-02, 3.2927e-02],\n",
      "        [1.8320e-02, 4.5410e-02, 9.8493e-03, 4.0978e-02, 1.2874e-02, 3.5316e-03,\n",
      "         1.2080e-02, 2.4438e-02],\n",
      "        [5.7405e-03, 2.1426e-02, 4.7490e-02, 4.6068e-02, 3.6307e-02, 2.6417e-02,\n",
      "         2.9567e-02, 2.8501e-02],\n",
      "        [2.9318e-02, 3.4085e-02, 7.3562e-03, 2.1544e-02, 2.7510e-02, 2.6116e-03,\n",
      "         4.8674e-02, 3.8176e-02],\n",
      "        [1.2915e-02, 8.0830e-05, 4.3429e-02, 4.7620e-03, 2.4293e-02, 4.4673e-02,\n",
      "         4.9043e-02, 7.1708e-03],\n",
      "        [1.7128e-02, 3.9223e-02, 3.4049e-02, 3.8373e-03, 3.0631e-02, 9.3797e-03,\n",
      "         2.9678e-02, 4.2427e-02],\n",
      "        [4.6779e-02, 4.9414e-02, 1.9940e-02, 7.2219e-03, 3.4205e-02, 3.5917e-02,\n",
      "         2.9003e-02, 4.2260e-03],\n",
      "        [4.7810e-02, 4.4387e-02, 3.7736e-02, 2.6386e-02, 2.2345e-02, 4.1284e-02,\n",
      "         3.3834e-02, 2.6497e-02],\n",
      "        [5.1959e-04, 1.5847e-02, 3.6053e-02, 4.9236e-02, 1.2686e-02, 3.9724e-02,\n",
      "         1.9652e-02, 3.6823e-02],\n",
      "        [4.5927e-02, 4.2016e-02, 4.3934e-02, 1.1039e-02, 2.8972e-02, 3.4830e-02,\n",
      "         1.8547e-02, 2.1276e-02],\n",
      "        [4.3702e-02, 4.2442e-02, 1.9716e-02, 3.0998e-02, 4.3951e-02, 3.0050e-02,\n",
      "         4.9637e-02, 1.1747e-02],\n",
      "        [4.5242e-02, 3.3965e-03, 3.6444e-03, 6.3124e-03, 7.6613e-03, 1.8245e-02,\n",
      "         1.6830e-02, 6.8281e-03],\n",
      "        [1.4277e-02, 4.1771e-02, 2.2410e-02, 3.0567e-02, 9.1539e-03, 3.3015e-02,\n",
      "         1.9560e-02, 4.4142e-04],\n",
      "        [1.9127e-02, 3.2417e-02, 4.8350e-03, 3.2097e-02, 2.3614e-02, 2.3505e-02,\n",
      "         4.2022e-02, 5.6971e-03],\n",
      "        [3.3076e-02, 4.8937e-02, 4.5087e-02, 3.2870e-02, 1.5303e-02, 4.3973e-02,\n",
      "         4.1968e-02, 1.3576e-02],\n",
      "        [6.2058e-03, 1.6151e-02, 5.4752e-03, 4.6963e-02, 3.2924e-02, 3.3432e-02,\n",
      "         4.2774e-02, 8.4489e-03],\n",
      "        [3.9847e-03, 3.5139e-02, 2.1174e-02, 2.1790e-03, 3.6670e-03, 1.5951e-02,\n",
      "         2.6205e-02, 3.6383e-02],\n",
      "        [1.1736e-02, 4.5720e-02, 3.5867e-02, 1.6669e-02, 4.8503e-02, 4.2523e-02,\n",
      "         3.4706e-02, 4.2144e-02],\n",
      "        [4.4797e-02, 3.8439e-02, 1.7800e-02, 4.1577e-02, 9.9798e-03, 3.4206e-03,\n",
      "         4.8884e-02, 2.3119e-02],\n",
      "        [9.2548e-03, 5.3865e-03, 2.4547e-02, 4.7510e-02, 4.1326e-02, 3.0995e-02,\n",
      "         3.4282e-02, 4.5683e-02],\n",
      "        [8.3252e-03, 1.1658e-02, 3.9869e-02, 2.3159e-02, 6.1540e-03, 7.2091e-03,\n",
      "         8.9717e-03, 4.7705e-02],\n",
      "        [3.6808e-02, 3.8566e-02, 1.5258e-02, 4.6440e-02, 4.4534e-02, 4.6695e-02,\n",
      "         4.6187e-02, 4.5400e-02],\n",
      "        [3.7175e-02, 9.0183e-03, 1.9450e-02, 2.4289e-02, 1.0740e-02, 1.3746e-02,\n",
      "         2.3955e-02, 3.7438e-02],\n",
      "        [8.2821e-03, 3.0216e-03, 2.4731e-03, 3.6866e-02, 1.3931e-02, 1.9904e-03,\n",
      "         3.7476e-02, 2.3979e-02],\n",
      "        [1.6029e-02, 8.8167e-03, 3.5113e-02, 3.0362e-02, 6.4800e-03, 7.3323e-03,\n",
      "         6.0684e-03, 1.8917e-02],\n",
      "        [2.2730e-02, 4.2312e-02, 2.7671e-02, 3.7737e-02, 4.6532e-02, 3.1621e-02,\n",
      "         9.3370e-03, 4.8642e-02],\n",
      "        [2.6175e-02, 4.1466e-02, 3.0200e-02, 9.0423e-03, 4.1042e-02, 2.5578e-03,\n",
      "         1.1010e-03, 1.0509e-02],\n",
      "        [4.0444e-02, 3.6520e-02, 2.4355e-02, 4.3058e-02, 3.4257e-02, 2.7538e-02,\n",
      "         2.8941e-02, 4.3202e-02],\n",
      "        [7.2730e-03, 7.0990e-03, 3.5214e-02, 7.1095e-03, 4.7858e-02, 3.5826e-02,\n",
      "         1.6391e-02, 3.3547e-02],\n",
      "        [1.4131e-02, 2.0898e-02, 3.0074e-02, 3.3440e-02, 3.4538e-02, 7.5320e-03,\n",
      "         7.3188e-03, 2.9746e-02],\n",
      "        [9.2998e-03, 3.4690e-02, 4.8422e-02, 4.0356e-02, 2.8205e-02, 1.2019e-02,\n",
      "         3.5505e-02, 4.0559e-02],\n",
      "        [2.2359e-02, 2.5438e-02, 2.2623e-02, 4.2178e-02, 8.4291e-04, 2.6594e-02,\n",
      "         4.8677e-03, 2.1591e-02],\n",
      "        [4.3651e-02, 1.4472e-02, 4.0724e-02, 2.1233e-02, 1.3951e-02, 2.1735e-02,\n",
      "         1.2552e-02, 3.8566e-02],\n",
      "        [4.5029e-03, 3.9495e-02, 2.0084e-02, 4.7540e-02, 4.7822e-02, 3.2834e-02,\n",
      "         1.8423e-03, 4.9509e-02]])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 128\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Is running on GPU:\", cuda)\n",
    "\n",
    "model = MatrixFactorization(n_users, n_items, n_factors=8)\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "# GPU enable if you have a GPU...\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "# The three steps to build a prototype: \n",
    "# 1) defining the model, \n",
    "# 2) defining the loss,\n",
    "# 3) and picking an optimization technique. The latter two steps are largely built into PyTorch, so we’ll start with the hardest first.\n",
    "\n",
    "# MSE loss (MEAN SQUARE ERROR: loss between the matrix factorization “prediction” and the actual user-item ratings)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "#adagrad_loss = torch.optim.Adagrad(model.parameters(), lr= 1e-3) #different optimization algorithm\n",
    "\n",
    "#With lr=1e-1 we have the lowest error at the last epoch\n",
    "# ADAM optimizier \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) #learnig rate\n",
    "\n",
    "# Train data\n",
    "train_set = Loader()\n",
    "train_loader = DataLoader(train_set, 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7632b6eb-c943-493f-a44e-d0d652dad5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makis\\AppData\\Local\\Temp/ipykernel_18132/1510178570.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for it in tqdm(range(num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc753258b65c4c5096608c61414e2a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter #0 Loss: 10.612371312992652\n",
      "iter #1 Loss: 9.105746975945085\n",
      "iter #2 Loss: 6.3355094002335886\n",
      "iter #3 Loss: 3.4893522128424683\n",
      "iter #4 Loss: 1.4786324491128897\n",
      "iter #5 Loss: 0.48493406896853386\n",
      "iter #6 Loss: 0.13831525898116934\n",
      "iter #7 Loss: 0.06237101252369411\n",
      "iter #8 Loss: 0.06050101955376013\n",
      "iter #9 Loss: 0.06650430211306685\n",
      "iter #10 Loss: 0.07306791317013218\n",
      "iter #11 Loss: 0.07685987026814152\n",
      "iter #12 Loss: 0.08791658001215867\n",
      "iter #13 Loss: 0.10127077803320592\n",
      "iter #14 Loss: 0.10284592001639364\n",
      "iter #15 Loss: 0.08925075715650684\n",
      "iter #16 Loss: 0.08453292143352502\n",
      "iter #17 Loss: 0.08543732386949422\n",
      "iter #18 Loss: 0.08750321596975216\n",
      "iter #19 Loss: 0.08612327395802569\n",
      "iter #20 Loss: 0.08404774722807548\n",
      "iter #21 Loss: 0.08163805295477437\n",
      "iter #22 Loss: 0.08103881657237896\n",
      "iter #23 Loss: 0.07967995588317552\n",
      "iter #24 Loss: 0.07916025585873658\n",
      "iter #25 Loss: 0.07744263784240579\n",
      "iter #26 Loss: 0.07676182403360181\n",
      "iter #27 Loss: 0.07510342906274454\n",
      "iter #28 Loss: 0.07455124867046276\n",
      "iter #29 Loss: 0.07319495327713545\n",
      "iter #30 Loss: 0.07254052770031078\n",
      "iter #31 Loss: 0.07119313463606798\n",
      "iter #32 Loss: 0.0705986202592054\n",
      "iter #33 Loss: 0.06926541760220857\n",
      "iter #34 Loss: 0.06876765461186009\n",
      "iter #35 Loss: 0.06757748674344072\n",
      "iter #36 Loss: 0.06709480985922886\n",
      "iter #37 Loss: 0.06567514555819352\n",
      "iter #38 Loss: 0.06535092568801493\n",
      "iter #39 Loss: 0.06418665436565724\n",
      "iter #40 Loss: 0.06371929982434148\n",
      "iter #41 Loss: 0.06257869585719712\n",
      "iter #42 Loss: 0.062184148780105974\n",
      "iter #43 Loss: 0.06113903919506408\n",
      "iter #44 Loss: 0.06077247368805396\n",
      "iter #45 Loss: 0.05974875666829936\n",
      "iter #46 Loss: 0.059338647314845144\n",
      "iter #47 Loss: 0.05829198967160471\n",
      "iter #48 Loss: 0.058001401341136766\n",
      "iter #49 Loss: 0.05694658623155578\n",
      "iter #50 Loss: 0.05675716582885788\n",
      "iter #51 Loss: 0.05578340699091135\n",
      "iter #52 Loss: 0.05546591588584206\n",
      "iter #53 Loss: 0.05460223143019945\n",
      "iter #54 Loss: 0.054325283059607384\n",
      "iter #55 Loss: 0.05337669208760152\n",
      "iter #56 Loss: 0.05318005403975391\n",
      "iter #57 Loss: 0.052356354349180866\n",
      "iter #58 Loss: 0.052063765771248764\n",
      "iter #59 Loss: 0.05121940332929344\n",
      "iter #60 Loss: 0.05095406785092848\n",
      "iter #61 Loss: 0.05019739905224584\n",
      "iter #62 Loss: 0.05001680315245905\n",
      "iter #63 Loss: 0.049247491399726596\n",
      "iter #64 Loss: 0.049024088017623445\n",
      "iter #65 Loss: 0.048259923329857915\n",
      "iter #66 Loss: 0.048047732013989895\n",
      "iter #67 Loss: 0.04730496363585715\n",
      "iter #68 Loss: 0.047133546779908796\n",
      "iter #69 Loss: 0.046461301905762815\n",
      "iter #70 Loss: 0.046241104421789386\n",
      "iter #71 Loss: 0.045551327616929094\n",
      "iter #72 Loss: 0.045496451699405985\n",
      "iter #73 Loss: 0.04480620132535315\n",
      "iter #74 Loss: 0.04459361382343275\n",
      "iter #75 Loss: 0.043960766823928986\n",
      "iter #76 Loss: 0.04381861806849537\n",
      "iter #77 Loss: 0.04320878654604068\n",
      "iter #78 Loss: 0.043035614906865007\n",
      "iter #79 Loss: 0.04239681899985846\n",
      "iter #80 Loss: 0.042296677089446344\n",
      "iter #81 Loss: 0.04174834649886012\n",
      "iter #82 Loss: 0.04159057337571593\n",
      "iter #83 Loss: 0.04102166244745864\n",
      "iter #84 Loss: 0.040867414053939184\n",
      "iter #85 Loss: 0.040345551026865954\n",
      "iter #86 Loss: 0.04024367380763411\n",
      "iter #87 Loss: 0.03971255378669028\n",
      "iter #88 Loss: 0.039536079772940984\n",
      "iter #89 Loss: 0.03902579318550999\n",
      "iter #90 Loss: 0.03891972749186751\n",
      "iter #91 Loss: 0.038485513080645094\n",
      "iter #92 Loss: 0.038343978140627025\n",
      "iter #93 Loss: 0.037847519016174405\n",
      "iter #94 Loss: 0.03774111470698243\n",
      "iter #95 Loss: 0.03729702029710688\n",
      "iter #96 Loss: 0.037142663508120094\n",
      "iter #97 Loss: 0.03674620909668753\n",
      "iter #98 Loss: 0.03662262942232287\n",
      "iter #99 Loss: 0.03617808857308629\n",
      "iter #100 Loss: 0.03603310522425662\n",
      "iter #101 Loss: 0.03563567578001781\n",
      "iter #102 Loss: 0.035500132786038585\n",
      "iter #103 Loss: 0.03506342295909782\n",
      "iter #104 Loss: 0.03500797671487417\n",
      "iter #105 Loss: 0.03462661992129691\n",
      "iter #106 Loss: 0.03452952934043182\n",
      "iter #107 Loss: 0.03411959796724722\n",
      "iter #108 Loss: 0.03404478210469951\n",
      "iter #109 Loss: 0.033628080073563035\n",
      "iter #110 Loss: 0.033510351353479774\n",
      "iter #111 Loss: 0.0331964236838967\n",
      "iter #112 Loss: 0.03312372690652642\n",
      "iter #113 Loss: 0.03270976344966675\n",
      "iter #114 Loss: 0.032643809359606424\n",
      "iter #115 Loss: 0.032290190948968955\n",
      "iter #116 Loss: 0.032199930935583605\n",
      "iter #117 Loss: 0.03184358225873364\n",
      "iter #118 Loss: 0.031773239443235846\n",
      "iter #119 Loss: 0.03142491460103742\n",
      "iter #120 Loss: 0.03135844513469035\n",
      "iter #121 Loss: 0.03106453631054181\n",
      "iter #122 Loss: 0.030989657558710373\n",
      "iter #123 Loss: 0.03063544478086407\n",
      "iter #124 Loss: 0.030484114947450132\n",
      "iter #125 Loss: 0.030246987900770534\n",
      "iter #126 Loss: 0.03021005439855482\n",
      "iter #127 Loss: 0.029877005087788146\n"
     ]
    }
   ],
   "source": [
    "#TQDM is used for creating Progress Meters or Progress Bars in Python.\n",
    "for it in tqdm(range(num_epochs)):\n",
    "    losses = []\n",
    "    for x, y in train_loader:\n",
    "         if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(\"iter #{}\".format(it), \"Loss:\", sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba51340d-d450-4357-9e97-65aaa53f8ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_factors.weight tensor([[0.1787, 0.1898, 0.1723,  ..., 0.1976, 0.1754, 0.1653],\n",
      "        [0.2602, 0.2574, 0.2483,  ..., 0.2311, 0.2198, 0.2338],\n",
      "        [0.2878, 0.2623, 0.2757,  ..., 0.2540, 0.2686, 0.2834],\n",
      "        ...,\n",
      "        [0.1285, 0.1364, 0.1275,  ..., 0.1112, 0.1330, 0.1124],\n",
      "        [0.2047, 0.2517, 0.2447,  ..., 0.2241, 0.2496, 0.2470],\n",
      "        [0.0721, 0.0898, 0.0913,  ..., 0.0953, 0.1054, 0.0712]],\n",
      "       device='cuda:0')\n",
      "item_factors.weight tensor([[1.9619, 1.9537, 1.9768, 1.9442, 1.9722, 1.9388, 1.9521, 1.9885],\n",
      "        [1.6227, 1.6262, 1.6414, 1.6565, 1.6317, 1.6540, 1.6584, 1.6623],\n",
      "        [2.2944, 2.2435, 2.2793, 2.2488, 2.2682, 2.2645, 2.2637, 2.2333],\n",
      "        [2.0625, 2.0695, 2.0722, 2.1038, 2.0890, 2.0969, 2.0908, 2.1062],\n",
      "        [2.0603, 2.0697, 2.0903, 2.0522, 2.0743, 2.0727, 2.0519, 2.1046],\n",
      "        [1.4712, 1.4734, 1.4931, 1.4934, 1.5190, 1.4736, 1.4995, 1.4697],\n",
      "        [2.1033, 2.1203, 2.1160, 2.0962, 2.1151, 2.1297, 2.1023, 2.1213],\n",
      "        [1.6507, 1.6757, 1.7015, 1.6676, 1.6739, 1.6901, 1.6702, 1.6952],\n",
      "        [1.7811, 1.7703, 1.7957, 1.7734, 1.7846, 1.8057, 1.7634, 1.8002],\n",
      "        [1.8451, 1.8506, 1.8662, 1.8596, 1.8378, 1.8173, 1.8537, 1.8628],\n",
      "        [1.8264, 1.8398, 1.8266, 1.8162, 1.8208, 1.8471, 1.8189, 1.8589],\n",
      "        [1.8079, 1.8120, 1.7917, 1.7926, 1.7998, 1.8393, 1.8038, 1.8064],\n",
      "        [1.8972, 1.9198, 1.9035, 1.9255, 1.9393, 1.9058, 1.9036, 1.8977],\n",
      "        [2.0836, 2.1064, 2.1273, 2.0788, 2.1016, 2.0848, 2.1011, 2.1233],\n",
      "        [1.8030, 1.7884, 1.8120, 1.8112, 1.8031, 1.7825, 1.8230, 1.8165],\n",
      "        [1.5587, 1.5255, 1.5534, 1.5224, 1.4889, 1.5032, 1.5303, 1.5019],\n",
      "        [2.0534, 2.0533, 2.0318, 2.0208, 2.0297, 2.0683, 2.0394, 2.0345],\n",
      "        [1.8592, 1.8674, 1.8497, 1.8582, 1.8604, 1.8813, 1.8800, 1.8655],\n",
      "        [1.7197, 1.7207, 1.7139, 1.7277, 1.7195, 1.7290, 1.7610, 1.7126],\n",
      "        [1.6961, 1.6700, 1.6814, 1.6632, 1.6697, 1.6994, 1.6887, 1.7200],\n",
      "        [1.6549, 1.6389, 1.6832, 1.6807, 1.6518, 1.6450, 1.6832, 1.6273],\n",
      "        [1.7005, 1.7628, 1.7197, 1.7492, 1.7601, 1.7206, 1.7643, 1.7436],\n",
      "        [1.9886, 2.0158, 2.0114, 2.0090, 2.0074, 1.9754, 2.0032, 1.9537],\n",
      "        [1.9938, 1.9621, 1.9954, 1.9940, 2.0084, 2.0263, 2.0046, 1.9954],\n",
      "        [1.8962, 1.8858, 1.8844, 1.9132, 1.8775, 1.8810, 1.8497, 1.8986],\n",
      "        [1.8567, 1.8645, 1.8353, 1.8287, 1.8392, 1.8339, 1.8386, 1.8145],\n",
      "        [1.8187, 1.8272, 1.8212, 1.7841, 1.7790, 1.8034, 1.8186, 1.8334],\n",
      "        [1.9865, 2.0127, 2.0140, 2.0126, 1.9657, 1.9711, 1.9800, 1.9911],\n",
      "        [1.8605, 1.9099, 1.8935, 1.8681, 1.8645, 1.8938, 1.8496, 1.8458],\n",
      "        [1.6828, 1.7015, 1.6477, 1.6850, 1.6903, 1.6939, 1.6736, 1.6552],\n",
      "        [1.7266, 1.7451, 1.7477, 1.7556, 1.7112, 1.7430, 1.7556, 1.7537],\n",
      "        [2.0506, 2.0694, 2.0778, 2.0663, 2.0594, 2.0502, 2.0763, 2.0879],\n",
      "        [1.9168, 1.9129, 1.9071, 1.8679, 1.9083, 1.9186, 1.9025, 1.9061],\n",
      "        [1.7714, 1.7657, 1.7572, 1.7555, 1.8023, 1.8260, 1.7995, 1.7665],\n",
      "        [1.9265, 1.9093, 1.9265, 1.9180, 1.9342, 1.9295, 1.9005, 1.9385],\n",
      "        [1.6623, 1.6543, 1.6449, 1.6553, 1.6967, 1.6703, 1.6600, 1.6441],\n",
      "        [1.8009, 1.7959, 1.7895, 1.8244, 1.7753, 1.7936, 1.8171, 1.7544],\n",
      "        [1.6159, 1.6136, 1.5965, 1.6337, 1.6337, 1.6086, 1.5965, 1.5966],\n",
      "        [1.9819, 1.9950, 1.9779, 1.9853, 1.9752, 1.9631, 1.9944, 1.9692],\n",
      "        [2.0759, 2.0916, 2.1066, 2.0417, 2.1171, 2.1049, 2.1104, 2.0718],\n",
      "        [1.8977, 1.9329, 1.9238, 1.9329, 1.8836, 1.9307, 1.9306, 1.9179],\n",
      "        [2.0434, 2.0832, 2.0546, 2.0745, 2.0814, 2.0681, 2.0867, 2.0269],\n",
      "        [1.8327, 1.8793, 1.8524, 1.8317, 1.8288, 1.8544, 1.8589, 1.8468],\n",
      "        [1.9476, 1.9634, 2.0005, 1.9671, 1.9569, 1.9752, 1.9353, 1.9378],\n",
      "        [1.9092, 1.9307, 1.9163, 1.9071, 1.9136, 1.9144, 1.9474, 1.9166],\n",
      "        [1.9337, 1.9578, 1.9699, 1.9707, 1.9488, 1.9708, 1.9769, 1.9568],\n",
      "        [1.7747, 1.7585, 1.8230, 1.7678, 1.7497, 1.7575, 1.7813, 1.7750],\n",
      "        [1.8640, 1.9116, 1.9024, 1.8964, 1.9243, 1.8952, 1.8903, 1.9272],\n",
      "        [1.8800, 1.8529, 1.8717, 1.8382, 1.8669, 1.8690, 1.8821, 1.8795],\n",
      "        [2.2386, 2.2429, 2.2149, 2.2465, 2.2470, 2.2030, 2.2396, 2.2244],\n",
      "        [2.0019, 1.9874, 2.0024, 1.9893, 1.9625, 2.0009, 1.9775, 1.9911],\n",
      "        [1.9365, 1.9570, 1.9410, 1.9579, 1.9807, 1.9716, 1.9466, 1.9837],\n",
      "        [1.8288, 1.8746, 1.8137, 1.8374, 1.8525, 1.8127, 1.8150, 1.8276],\n",
      "        [1.5571, 1.6336, 1.5894, 1.6163, 1.5797, 1.5477, 1.5957, 1.6476],\n",
      "        [1.7581, 1.7704, 1.7353, 1.7152, 1.8191, 1.7794, 1.7328, 1.7793],\n",
      "        [1.8213, 1.7942, 1.7985, 1.7799, 1.7722, 1.8134, 1.7948, 1.7506],\n",
      "        [1.7546, 1.7786, 1.8313, 1.8259, 1.8106, 1.7875, 1.8055, 1.7729],\n",
      "        [1.6070, 1.6121, 1.5899, 1.5971, 1.6113, 1.6092, 1.5933, 1.6213],\n",
      "        [1.8803, 1.8600, 1.8742, 1.8796, 1.8902, 1.8605, 1.8801, 1.8974],\n",
      "        [1.9595, 1.9774, 1.9653, 1.9613, 1.9313, 1.9442, 1.9298, 1.9644]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# By training the model, we will have tuned latent factors for products and products. \n",
    "# Latent variable is a variable that cannot be observed. \n",
    "# The presence of latent variables, however, can be detected by their effects on variables that are observable\n",
    "\n",
    "c = 0\n",
    "uw = 0\n",
    "iw = 0 \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "        if c == 0:\n",
    "            uw = param.data\n",
    "            c +=1\n",
    "        else:\n",
    "            iw = param.data\n",
    "        #print('param_data', param_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9311e5b-5dd0-4bba-b91f-d1b3d30d4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_products_embeddings = model.item_factors.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2a23412-337f-4031-b817-8d6d106174a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trained_products_embeddings) # Unique products factor weights !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c295d43-8bcf-410a-97ce-478ed024ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Fit the clusters based on the products weights\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(trained_products_embeddings) #Orismos CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6e4a5ee-482b-4355-9863-e70e32c18924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0\n",
      "\t Samsung Galaxy S21\n",
      "\t Xiaomi 12X\n",
      "\t Samsung Galaxy S21 Ultra\n",
      "\t One Plus Nord 2\n",
      "\t Xiaomi Redmi Note 10\n",
      "\t Samsung Galaxy S20\n",
      "\t Huawei Nova 9\n",
      "\t Xiaomi Redmi Note 10 Pro\n",
      "\t Google Pixel 6 Pro\n",
      "\t Iphone 12\n",
      "Cluster #1\n",
      "\t One Plus 8 Pro\n",
      "\t Iphone 13 Mini\n",
      "\t Iphone 13 Pro\n",
      "\t Huawei 8i\n",
      "\t Xiaomi Redmi Note 11\n",
      "\t Samsung Galaxy S22 Plus\n",
      "\t Iphone 11 Pro Max\n",
      "\t Google Pixel 6a\n",
      "\t Realme 9 Pro Plus\n",
      "\t Realme GT Neo 2\n",
      "Cluster #2\n",
      "\t Iphone 11 Pro\n",
      "\t Google Pixel 5a\n",
      "\t Samsung Galaxy S20 Fe\n",
      "\t Xiaomi Poco F3\n",
      "\t Huawei P40 lite\n",
      "\t Realme GT 2 Pro\n",
      "\t Huawei P50 Pocket\n",
      "\t One Plus 10 Pro\n",
      "\t Realme Gt Neo 2\n",
      "\t Huawei P30\n",
      "Cluster #3\n",
      "\t Xiaomi Poco M4 Pro\n",
      "\t Xiaomi Redmi Note 11 Pro\n",
      "\t Iphone 12 Mini\n",
      "\t Huawei P50\n",
      "\t Huawei P40\n",
      "\t Samsung Galaxy S20 Plus\n",
      "\t One Plus 8T\n",
      "\t Realme 9 Pro\n",
      "\t Google Pixel 5\n",
      "\t Iphone 12 Pro\n"
     ]
    }
   ],
   "source": [
    "# It can be seen here that the products that are in the same cluster tend to have\n",
    "# similar categories.\n",
    "# Also note that the algorithm is unfamiliar with the product name\n",
    "# and only obtained the relationships by looking at the numbers representing how\n",
    "# users have responded to the products selections.\n",
    "\n",
    "\n",
    "#In our situation we decided to it for 4 Cluster as our Categories \n",
    "\n",
    "for cluster in range(4):\n",
    "    print(\"Cluster #{}\".format(cluster))\n",
    "    prods = []\n",
    "    for prodidx in np.where(kmeans.labels_ == cluster)[0]:\n",
    "        prodid = train_set.idx2productid[prodidx]\n",
    "        rat_count = ratings_df.loc[ratings_df['productId']==prodid].count()[0]\n",
    "        prods.append((products_names[prodid], rat_count))\n",
    "    for prod in sorted(prods, key=lambda tup: tup[1], reverse=True)[:10]:\n",
    "        print(\"\\t\", prod[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0830154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
